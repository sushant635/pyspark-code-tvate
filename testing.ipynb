{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/11 21:06:59 WARN Utils: Your hostname, sushants-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.1.93 instead (on interface en0)\n",
      "25/03/11 21:06:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/11 21:06:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/03/11 21:06:59 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import input_file_name, row_number,when,col,min,max,unix_timestamp,rank, dense_rank,date_format,count,lead,lag,sum,expr,from_utc_timestamp,substring,abs,from_unixtime,to_date,greatest,to_timestamp,lit,concat,split,current_date, concat_ws,regexp_extract,lpad,first, last,date_add,hour,coalesce,floor,window\n",
    "from functools import reduce\n",
    "from pyspark.sql.window import Window\n",
    "spark = SparkSession.builder.appName(\"MulRead\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df2_block = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"/Users/sushantshinde/workspaces/trvi/blockformer8-9/*.csv\").withColumn(\"join_date\", date_format(col(\"_time\"), \"yyyy-MM-dd HH:mm:ss\").cast(\"timestamp\"))\\\n",
    "    .withColumnRenamed(\"B1D1.AI_D1B31\",\"AI_D1B31\").withColumnRenamed(\"B1D1.AI_D1B32\",\"AI_D1B32\").withColumnRenamed(\"B1D1.AI_D1B34\",\"AI_D1B34\")\\\n",
    "    # .filter(col(\"join_date\").cast(\"timestamp\").between('2025-03-08T22:00:00','2025-03-09T22:00:00'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert _time to timestamp\n",
    "df2_block = df2_block.withColumn(\"_time\", to_timestamp(\"_time\"))\n",
    "\n",
    "# Create a window to track changes\n",
    "window = Window.orderBy(\"_time\")\n",
    "\n",
    "# Use lag to compare current and previous values\n",
    "df2_block = df2_block.withColumn(\"prev_valueblock1\", lag(\"IO_MI_B1D1C3cut\").over(window))\n",
    "df2_block = df2_block.withColumn(\"prev_prev_valueblock1\", lag(\"IO_MI_B1D1C3cut\", 3).over(window))\n",
    "\n",
    "\n",
    "# Create a new column 'start_time' in the original DataFrame\n",
    "df2_block = df2_block.withColumn(\n",
    "    \"end_time_block1\",\n",
    "    # when(\n",
    "    #     (col(\"IO_MI_B1D1C3cut\") == 1) & (col(\"prev_value\").isNull() | (col(\"prev_value\") == 0)),\n",
    "    #     col(\"_time\")\n",
    "    # ).otherwise(None)\n",
    "     when(\n",
    "        (col(\"IO_MI_B1D1C3cut\") == 0) & (col(\"prev_valueblock1\") == 1) \n",
    "        & (col('prev_prev_valueblock1') == 1)\n",
    "        ,\n",
    "        col(\"_time\")\n",
    "    ).otherwise(None)\n",
    ")\n",
    "\n",
    "df2_block = df2_block.withColumn(\"prev_valueblock2\", lag(\"IO_MI_B1D2C3cut\").over(window))\n",
    "df2_block = df2_block.withColumn(\"prev_prev_valueblock2\", lag(\"IO_MI_B1D2C3cut\", 3).over(window))\n",
    "\n",
    "\n",
    "# Create a new column 'start_time' in the original DataFrame\n",
    "df2_block = df2_block.withColumn(\n",
    "    \"end_time_block2\",\n",
    "    # when(\n",
    "    #     (col(\"IO_MI_B1D1C3cut\") == 1) & (col(\"prev_value\").isNull() | (col(\"prev_value\") == 0)),\n",
    "    #     col(\"_time\")\n",
    "    # ).otherwise(None)\n",
    "     when(\n",
    "        (col(\"IO_MI_B1D2C3cut\") == 0) & (col(\"prev_valueblock2\") == 1 ) \n",
    "        & (col('prev_prev_valueblock2') == 1)\n",
    "        ,\n",
    "        col(\"_time\")\n",
    "    ).otherwise(None)\n",
    ")\n",
    "\n",
    "# df2_block = df2_block.drop('prev_valueblock2','prev_valueblock1','prev_prev_valueblock1','prev_prev_valueblock2')\n",
    "\n",
    "# Fill null values in 'start_time' column\n",
    "# window_fill = Window.orderBy(\"_time\").rowsBetween(Window.currentRow, Window.unboundedFollowing)\n",
    "# df_with_start_time = df_with_start_time.withColumn(\n",
    "#     \"start_time\",\n",
    "#     last(\"start_time\", True).over(window_fill)\n",
    "# )\n",
    "\n",
    "# Display the DataFrame with start times\n",
    "# df2_block.show(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_join3 = df2_block.withColumn('BlockEndTIme',\n",
    "                                   when(col('end_time_block1').isNotNull(),col('end_time_block1'))\n",
    "                                   .when(col('end_time_block2').isNotNull(),col('end_time_block2')).otherwise(None)\n",
    "                                   )\n",
    "\n",
    "\n",
    "df = final_join3.withColumn(\"BlockEndTIme\", col(\"BlockEndTIme\").cast(\"timestamp\"))\n",
    "window_spec = Window.orderBy(\"BlockEndTIme\")\n",
    "\n",
    "df = df.withColumn(\"next_datetime\", lead(\"BlockEndTIme\").over(window_spec))\n",
    "\n",
    "df = df.withColumn(\"time_diff_hours\", \n",
    "                   when(\n",
    "                       col(\"next_datetime\").isNotNull(), \n",
    "                       (unix_timestamp(col(\"next_datetime\")) - unix_timestamp(col(\"BlockEndTIme\"))) / 3600\n",
    "                   ).otherwise(0)  \n",
    ")\n",
    "\n",
    "df = df.withColumn(\"is_new_block\", when(col(\"time_diff_hours\") > 3, 1).otherwise(0))\n",
    "window_spec_cumsum = Window.orderBy(\"BlockEndTIme\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "df = df.withColumn(\"block_id\", sum(\"is_new_block\").over(window_spec_cumsum) + 1)\n",
    "# df = df.withColumn(\"batch_id\", sum(\"is_new_batch\").over(window_spec_cumsum) + 1)\n",
    "\n",
    "\n",
    "df =  df.withColumn('BlockFomerId',\n",
    "                                   when(col('end_time_block1').isNotNull(),'BlockFormer1')\n",
    "                                   .when(col('end_time_block2').isNotNull(),'BlockFormer2').otherwise(None)\n",
    "                                   )\n",
    "df = df.withColumn(\"block_id\", \n",
    "                   when(\n",
    "                       col(\"is_new_block\") == 1, \n",
    "                        col(\"block_id\") - 1  # Assign the previous block_id to the first row of the new block\n",
    "                   ).otherwise(col(\"block_id\"))\n",
    ")\n",
    "# .filter(col('block_id') == 2)\n",
    "df = df.drop(\"next_datetime\", \"time_diff_hours\", \"is_new_block\")\n",
    "\n",
    "# df = df.filter(col('join_date').between('2025-03-08 22:12:28','2025-03-09 21:29:30'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/11 21:52:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/11 21:52:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/11 21:52:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/11 21:52:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/11 21:52:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/11 21:52:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/11 21:52:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/11 21:52:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/11 21:52:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/11 21:52:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/11 21:52:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/11 21:52:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-------------------+-------------------+\n",
      "|block_id|min(join_date)     |max(join_date)     |count(BlockFomerId)|\n",
      "+--------+-------------------+-------------------+-------------------+\n",
      "|1       |2025-03-07 23:04:29|2025-03-11 09:31:48|2151               |\n",
      "|2       |2025-03-09 01:23:21|2025-03-09 21:29:30|2106               |\n",
      "|3       |2025-03-10 01:31:36|2025-03-10 01:52:35|8                  |\n",
      "|4       |2025-03-10 05:28:01|2025-03-10 14:17:57|823                |\n",
      "|5       |2025-03-11 08:21:46|2025-03-11 09:31:25|142                |\n",
      "+--------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('block_id').agg(min('join_date'),max('join_date'),count('BlockFomerId')).show(truncate=False)\n",
    "# df.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/11 21:52:59 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/11 21:52:59 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/11 21:52:59 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/11 21:52:59 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/11 21:52:59 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/11 21:52:59 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/11 21:53:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/11 21:53:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/11 21:53:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/11 21:53:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/11 21:53:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/11 21:53:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "blob_path = '/Users/sushantshinde/workspaces/trvi/output'\n",
    "df.repartition(1).orderBy('_time') \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .csv(blob_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read all CSV files in the folder\n",
    "alfomatic_path = '/Users/sushantshinde/workspaces/trvi/alfomatic8-9/*.csv'\n",
    "windowSpec_csv_filename = Window.partitionBy('FileName').orderBy(\"FileName\")\n",
    "df_alf = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(alfomatic_path).withColumn(\"FileName\", input_file_name()).withColumn(\"join_date\", date_format(col(\"_time\"), \"yyyy-MM-dd HH:mm:ss\").cast(\"timestamp\"))\\\n",
    "    .withColumn(\"FileName\", input_file_name())\\\n",
    "    .withColumn(\"file_id\", dense_rank().over(Window.orderBy(\"FileName\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ids = df_alf.withColumn('VatNo1',when((col('V_G20EMP_N') == 1) | (col('V_G20EMP_N_1') == 1), col('_time')))\\\n",
    "                .withColumn('VatNo2',when((col('V_G20EMP_N') == 2) | (col('V_G20EMP_N_1') == 2), col('_time')))\\\n",
    "                .withColumn('VatNo3',when((col('V_G20EMP_N') == 3) | (col('V_G20EMP_N_1') == 3), col('_time')))\\\n",
    "                .withColumn('VatNo4',when((col('V_G20EMP_N') == 4) | (col('V_G20EMP_N_1') == 4), col('_time')))\\\n",
    "                .withColumn('VatNo5',when((col('V_G20EMP_N') == 5) | (col('V_G20EMP_N_1') == 5), col('_time')))\\\n",
    "                .withColumn('VatNo6',when((col('V_G20EMP_N') == 6) | (col('V_G20EMP_N_1') == 6), col('_time')))\\\n",
    "                .withColumn('VatNo7',when((col('V_G20EMP_N') == 7) | (col('V_G20EMP_N_1') == 7), col('_time')))\\\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ids = df_ids.withColumn('VATEndTIme',\n",
    "                                   when(col('VatNo1').isNotNull(),col('VatNo1'))\n",
    "                                   .when(col('VatNo2').isNotNull(),col('VatNo2'))\n",
    "                                   .when(col('VatNo3').isNotNull(),col('VatNo3'))\n",
    "                                   .when(col('VatNo4').isNotNull(),col('VatNo4'))\n",
    "                                   .when(col('VatNo5').isNotNull(),col('VatNo5'))\n",
    "                                   .when(col('VatNo6').isNotNull(),col('VatNo6'))\n",
    "                                   .when(col('VatNo7').isNotNull(),col('VatNo7'))\n",
    "                                   .otherwise(None)\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/11 11:35:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/11 11:35:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/11 11:35:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/11 11:35:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# df_group = final_join3.groupBy('BlockFomerId','file_id','V_G20EMP_N').agg(count(\"*\").alias('blocks count')).filter((col('BlockFomerId').isNotNull()) & (col('V_G20EMP_N') != 0))\\\n",
    "blob_path = '/Users/sushantshinde/workspaces/trvi/output'\n",
    "df_ids.repartition(1).orderBy('_time') \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .csv(blob_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
