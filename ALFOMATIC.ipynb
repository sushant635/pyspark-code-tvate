{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import input_file_name, row_number,when,col,min,max,unix_timestamp,rank, dense_rank,date_format,count,lead,lag,sum,expr,from_utc_timestamp,substring,abs,from_unixtime,to_date,greatest,to_timestamp,lit,concat,split,current_date, concat_ws,regexp_extract,lpad,first, last,date_add,hour,coalesce,floor,window\n",
    "from pyspark.sql.window import Window\n",
    "spark = SparkSession.builder.appName(\"MulRead\").getOrCreate()\n",
    "# spark.conf.set(f\"fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\", sas_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"/Users/sushantshinde/workspaces/trvi/ALFOMATIC/*.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all CSV files in the folder\n",
    "windowSpec_csv_filename = Window.partitionBy('FileName').orderBy(\"FileName\")\n",
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(folder_path).withColumn(\"FileName\", input_file_name()).withColumn(\"join_date\", date_format(col(\"_time\"), \"yyyy-MM-dd HH:mm:ss\"))\\\n",
    "    .withColumn(\"FileName\", input_file_name())\\\n",
    "    .withColumn(\"file_id\", dense_rank().over(Window.orderBy(\"FileName\")))\n",
    "    # .withColumn(\"date_only\", to_date(col(\"_start\")))\\\n",
    "    # .withColumn(\"date_id\", dense_rank().over(Window.orderBy('date_only')))\\\n",
    "    # .withColumn(\"FileName\", regexp_extract(col(\"FileName\"), r\"([^/]+)$\", 1)) \\\n",
    "    # .withColumn(\"file_date\", substring(col(\"FileName\"), 1, 8)).withColumn(\"date_id\", concat(lit(\"ID_\"), col(\"file_date\")))\\\n",
    "    # .withColumn(\"date_id\", expr(\"CAST(SUBSTRING(date_id, 4, LENGTH(date_id)) AS INT)\"))\n",
    "    # .withColumn(\"_time\", from_utc_timestamp(col(\"_time\"), \"Asia/Kolkata\")).withColumn(\"_stop\", from_utc_timestamp(col(\"_stop\"), \"Asia/Kolkata\")).withColumn(\"_start\", from_utc_timestamp(col(\"_start\"), \"Asia/Kolkata\"))\\\n",
    "    # \n",
    "\n",
    "    # .withColumn(\"file_date\", substring(col(\"FileName\"), 1, 8))\\\n",
    "#    .withColumn(\"date_id\", concat(lit(\"ID_\"), col(\"file_date\")))\\ dinesgji banas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df2 = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"/Users/sushantshinde/workspaces/trvi/plc08-11/*.csv\").withColumn(\"join_date\", date_format(col(\"_time\"), \"yyyy-MM-dd HH:mm:ss\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join = df.alias('df').join(df2.alias('df2'), on=\"join_date\", how=\"left\").select('df.*','df2.Report_Belt1_Speed','df2.Report_Belt2_Speed','df2.Report_Belt2LanceFlow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group_df = df_join.groupBy(col('V_G20EMP_N'),col('file_id')).agg(date_format(min(col(\"_time\")).cast(\"timestamp\"),\"HH:mm:ss\").alias('Start_Time'),date_format(max(col(\"_time\")).cast(\"timestamp\"), \"HH:mm:ss\").alias('Stop_Time'),max('Report_Belt1_Speed').alias('speed'),max('Report_Belt2_Speed').alias('lower speed'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# window_spec = Window.partitionBy(\"V_G20EMP_N\", \"start_time\").orderBy(col('Stop_Time').desc())\n",
    "\n",
    "\n",
    "# df_result = group_df.alias('df1').join(group_df.alias('df2'), (col('df1.V_G20EMP_N') == col('df2.V_G20EMP_N')) & (col('df1.file_id') == col('df2.file_id') + 1),'left')\\\n",
    "# .select('df1.V_G20EMP_N','df1.file_id','df1.speed','df1.lower speed',when(col(\"df2.Start_Time\").isNotNull(), col(\"df2.Start_Time\")).otherwise(col(\"df1.Start_Time\")).alias('start_time'),'df1.Stop_Time').filter(col('V_G20EMP_N') != 0)\\\n",
    "# .withColumn(\"row_num\", row_number().over(window_spec)).filter(col('row_num') == 1).drop(\"row_num\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_result.orderBy('df1.file_id').show(df_result.count(),truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group_df_end = df_join.groupBy(col('V_G20EMP_N_1'),col('file_id')).agg(date_format(min(col(\"_time\")).cast(\"timestamp\"),\"HH:mm:ss\").alias('Start_Time'),date_format(max(col(\"_time\")).cast(\"timestamp\"), \"HH:mm:ss\").alias('Stop_Time')).filter(col('V_G20EMP_N_1') != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# window_spe2 = Window.partitionBy(\"V_G20EMP_N_1\", \"start_time\").orderBy(col('Stop_Time').desc())\n",
    "\n",
    "# df_result_end = group_df_end.alias('df1').join(group_df_end.alias('df2'), (col('df1.V_G20EMP_N_1') == col('df2.V_G20EMP_N_1')) & (col('df1.file_id') == col('df2.file_id') + 1),'left')\\\n",
    "# .select('df1.V_G20EMP_N_1','df1.file_id',when(col(\"df2.Start_Time\").isNotNull(), col(\"df2.Start_Time\")).otherwise(col(\"df1.Start_Time\")).alias('start_time'),'df1.Stop_Time')\\\n",
    "# .withColumn(\"row_num\", row_number().over(window_spe2)).filter(col('row_num') == 1).drop(\"row_num\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_result_end.orderBy('file_id').show(df_result_end.count(),truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_join_cooking_df =  df_result.alias('df1').join(df_result_end.alias('df2'), (col('df1.V_G20EMP_N') == col('df2.V_G20EMP_N_1')) &         \n",
    "#             (date_format(col(\"df1.Stop_Time\"), \"HH:mm\") == date_format(col(\"df2.start_time\"), \"HH:mm\")),'left').select('V_G20EMP_N','df1.start_time','df2.Stop_Time','speed','df1.lower speed','df1.file_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_final_test = df_join_cooking_df.withColumn(\"upper_belft_stop\", date_format(to_timestamp(col(\"Stop_Time\"), \"HH:mm:ss\") + expr(\"INTERVAL 30 MINUTES\"), \"HH:mm:ss\")\n",
    "# ).withColumn(\"lower_belft_stop\", date_format(to_timestamp(col(\"upper_belft_stop\"), \"HH:mm:ss\") + expr(\"INTERVAL 60 MINUTES\"), \"HH:mm:ss\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when(col(\"df2.Cooking_Stop_Time\").isNotNull(), col(\"df2.Cooking_Stop_Time\")).otherwise(col(\"df1.Cooking_Stop_Time\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group_df.orderBy('file_id').show(group_df.count(),truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = spark.read.format(\"csv\") \\\n",
    "#     .option(\"header\", \"true\") \\\n",
    "#     # .option(\"inferSchema\", \"true\") \n",
    "#     .load(\"/Users/sushantshinde/workspaces/trvi/plc08-11/*.csv\").withColumn(\"join_date\", date_format(col(\"_time\"), \"yyyy-MM-dd HH:mm:ss\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_join = df.alias('df').join(df2.alias('df2'), on=\"join_date\", how=\"left\").select('df.*','df2.Report_Belt1_Speed','df2.Report_Belt2_Speed','df2.Report_Belt2LanceFlow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_join.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blob_path = '/Users/sushantshinde/workspaces/trvi/output'\n",
    "# df_final_test.repartition(1).orderBy('file_id') \\\n",
    "#     .write \\\n",
    "#     .mode(\"overwrite\") \\\n",
    "#     .option(\"header\", True) \\\n",
    "#     .csv(blob_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blob_path = '/Users/sushantshinde/workspaces/trvi/output'\n",
    "# df.repartition(1).orderBy('_time') \\\n",
    "#     .write \\\n",
    "#     .mode(\"overwrite\") \\\n",
    "#     .option(\"header\", True) \\\n",
    "#     .csv(blob_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upper_df = df_join.select('_start','_stop','_time','_measurement','V_F03LENG','AI_D50B66','V_G20EMP_N','V_G20EMP_N_1','V_G23SPEED_N','V_G23SPEED_N_1','Report_Belt1_Speed')\\\n",
    "# .withColumn('volumn',col('V_F03LENG') * col('AI_D50B66') * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower_df= df_join.select('_start','_stop','_time','_measurement','V_F07LENG','V_G01HGHT1','V_G02SALT1','V_H80SETP','V_G20EMP_N','V_G20EMP_N_1','V_G23SPEED_N','V_G23SPEED_N_1','Report_Belt2_Speed')\\\n",
    "# .withColumn('volumn',col('V_F07LENG') * col('V_G01HGHT1') * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blob_path = '/Users/sushantshinde/workspaces/trvi/output'\n",
    "# upper_df.repartition(1).orderBy('_time') \\\n",
    "#     .write \\\n",
    "#     .mode(\"overwrite\") \\\n",
    "#     .option(\"header\", True) \\\n",
    "#     .csv(blob_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# import glob\n",
    "\n",
    "# output_folder = \"/Users/sushantshinde/workspaces/trvi/output\"\n",
    "# csv_files = glob.glob(f\"{output_folder}/part-*.csv\")\n",
    "\n",
    "# if csv_files:\n",
    "#     shutil.move(csv_files[0], \"/Users/sushantshinde/workspaces/trvi/output_csv/upper_belt-data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ids = df_join.withColumn('VatNo1',when((col('V_G20EMP_N') == 1) | (col('V_G20EMP_N_1') == 1), col('_time')))\\\n",
    "                .withColumn('VatNo2',when((col('V_G20EMP_N') == 2) | (col('V_G20EMP_N_1') == 2), col('_time')))\\\n",
    "                .withColumn('VatNo3',when((col('V_G20EMP_N') == 3) | (col('V_G20EMP_N_1') == 3), col('_time')))\\\n",
    "                .withColumn('VatNo4',when((col('V_G20EMP_N') == 4) | (col('V_G20EMP_N_1') == 4), col('_time')))\\\n",
    "                .withColumn('VatNo5',when((col('V_G20EMP_N') == 5) | (col('V_G20EMP_N_1') == 5), col('_time')))\\\n",
    "                .withColumn('VatNo6',when((col('V_G20EMP_N') == 6) | (col('V_G20EMP_N_1') == 6), col('_time')))\\\n",
    "                .withColumn('VatNo7',when((col('V_G20EMP_N') == 7) | (col('V_G20EMP_N_1') == 7), col('_time')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Assign row numbers and batch IDs\n",
    "# window_spec = Window.orderBy(\"_time\")\n",
    "# df2 = df2.withColumn(\"row_num\", row_number().over(window_spec))\n",
    "# df2 = df2.withColumn(\"batch_id\", floor((col(\"row_num\") - 1) / 30))  # Every 30 records = 1 batch\n",
    "\n",
    "# # Step 2: Get the batch start time (Earliest _time in each batch)\n",
    "# batch_window = Window.partitionBy(\"batch_id\").orderBy(\"_time\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "# df2 = df2.withColumn(\"batch_start_time\", min(\"_time\").over(batch_window))\n",
    "\n",
    "# # Step 3: Extract HHmmss time window\n",
    "# df2 = df2.withColumn(\"time_window\", date_format(col(\"batch_start_time\"), \"HHmmss\"))\n",
    "\n",
    "# # Step 4: Generate upper belt ID\n",
    "# vat_columns = [f\"VatNo{i}\" for i in range(1, 8)]\n",
    "# df2 = df2.withColumn(\"id\", \n",
    "#                    concat_ws(\"\", \n",
    "#                              *[when(col(vat).isNotNull(), lit(f\"vat{i}_\"))\n",
    "#                              .otherwise(lit(\"\")) for i, vat in enumerate(vat_columns, 1)],\n",
    "#                              col(\"time_window\")\n",
    "#                    ))\n",
    "# df2 = df2.withColumn(\"upper_belt_id\", concat_ws(\"_\", col(\"id\"), col(\"batch_id\")))\n",
    "\n",
    "# # Step 5: Calculate the lower belt start time (exactly 30 minutes after upper belt start)\n",
    "# df2 = df2.withColumn(\"lower_belt_start_time\", col(\"batch_start_time\") + expr(\"INTERVAL 30 MINUTES\"))\n",
    "\n",
    "# # Step 6: Find the row where _time matches the calculated lower belt start time\n",
    "# df_lower = df2.select(\"_time\", \"upper_belt_id\").withColumnRenamed(\"_time\", \"lower_belt_start_time\")\n",
    "\n",
    "# df_lower = df_lower.select(\"lower_belt_start_time\", col(\"upper_belt_id\").alias(\"temp_lower_belt_id\"))\n",
    "\n",
    "# df2 = df2.join(df_lower, on=\"lower_belt_start_time\", how=\"left\")\n",
    "\n",
    "# df2 = df2.withColumn(\"lower_belt_id\", when(col(\"temp_lower_belt_id\").isNotNull(), col(\"temp_lower_belt_id\")).otherwise(None)) \\\n",
    "#          .drop(\"temp_lower_belt_id\")\n",
    "# # Show the result\n",
    "# df2.select(\"_time\", \"upper_belt_id\", \"lower_belt_start_time\", \"lower_belt_id\").show(50, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_spec = Window.orderBy(\"_time\")\n",
    "df_ids = df_ids.withColumn(\"row_num\", row_number().over(window_spec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ids = df_ids.withColumn(\"batch_id\", floor((col(\"row_num\") - 1) / 31))\n",
    "df_ids = df_ids.withColumn(\"time_window\", date_format(col(\"_time\"), \"HHmmss\"))\n",
    "\n",
    "vat_columns = [f\"VatNo{i}\" for i in range(1, 8)]  # vat_1_date to vat_7_date\n",
    "df2 = df_ids.withColumn(\"id\", \n",
    "                   concat_ws(\"\", \n",
    "                             *[when(col(vat).isNotNull(), lit(f\"vat{i}_\"))\n",
    "                             .otherwise(lit(\"\")) for i, vat in enumerate(vat_columns, 1)],\n",
    "                             col(\"time_window\")\n",
    "                   ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ids.select('VatNo1','VatNo2','VatNo3','VatNo4','VatNo5','VatNo6','VatNo7','_time','time_window','batch_start_time','batch_time_str','id').show(50,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove empty strings from the ID\n",
    "df2 = df2.withColumn(\"id\", \n",
    "                   concat_ws(\"\", \n",
    "                             *[col(\"id\").substr(i, 4) for i in range(1, 29, 4)]\n",
    "                   ))\n",
    "\n",
    "df2 = df2.withColumn(\"upper_belt_id\", concat_ws(\"_\", col(\"id\"), col(\"batch_id\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate lower belt start time (30 minutes after upper belt start time)\n",
    "df2 = df2.withColumn(\"lower_belt_time\", expr(\"cast(_time as timestamp) + INTERVAL 30 MINUTES\"))\n",
    "\n",
    "# Generate lower belt ID\n",
    "df2 = df2.withColumn(\"lower_belt_time_window\", date_format(col(\"lower_belt_time\"), \"HHmmss\"))\n",
    "window_spec = Window.orderBy(\"_time\")\n",
    "\n",
    "df2 = df2.withColumn(\"lower_belt_id\", \n",
    "                   concat_ws(\"\", \n",
    "                             *[when(col(vat).isNotNull(), lit(f\"vat{i}_\"))\n",
    "                             .otherwise(lit(\"\")) for i, vat in enumerate(vat_columns, 1)],\n",
    "                             col(\"lower_belt_time_window\")\n",
    "                   ))# df2.select(\"_time\", \"upper_belt_id\", \"lower_belt_time\", \"lower_belt_id\").show(truncate=False)\n",
    "\n",
    "df2 = df2.withColumn(\"lower_belt_id\", \n",
    "                   concat_ws(\"\", \n",
    "                             *[col(\"lower_belt_id\").substr(i, 4) for i in range(1, 29, 4)]\n",
    "                   ))\n",
    "\n",
    "df2 = df2.withColumn(\"lower_belt_id\", concat_ws(\"_\", col(\"lower_belt_id\"), col(\"batch_id\")))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df2 = df2.withColumn(\"lower_belt_id\", col(\"lower_belt_id\"))\n",
    "\n",
    "\n",
    "# Show the result\n",
    "# df2.select(\"_time\", \"upper_belt_id\", \"lower_belt_time\", \"lower_belt_id\").show(100, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.withColumn(\"vat_1\", split(col(\"upper_belt_id\"), \"_\").getItem(0))\n",
    "\n",
    "df2 = df2.withColumn(\"lower_belt_window\", window(col(\"lower_belt_time\"), \"15 seconds\"))\n",
    "df2 = df2.withColumn(\"vat_1\", split(col(\"upper_belt_id\"), \"_\").getItem(0))\n",
    "# Step 3: Generate the `HHmmss` part from `lower_belt_time`\n",
    "# df2 = df2.withColumn(\"time_part\", date_format(col(\"lower_belt_time\"), \"HHmmss\"))\n",
    "\n",
    "# Step 4: Extract `batch_id` from `upper_belt_id`\n",
    "df2 = df2.withColumn(\"batch_id\", split(col(\"upper_belt_id\"), \"_\").getItem(2))\n",
    "\n",
    "\n",
    "\n",
    "# Step 5: Construct `lower_belt_id` by combining `vat_1`, `time_part`, and `batch_id`\n",
    "df2 = df2.withColumn(\"lower_belt_id\", \n",
    "                   concat_ws(\"_\", \n",
    "                             col(\"vat_1\"),  # Extract vat_1 from upper_belt_id\n",
    "                             date_format(col(\"lower_belt_window.start\"), \"HHmmss\"),  # Window start time\n",
    "                             col(\"batch_id\")  # Include batch_id\n",
    "                   ))\n",
    "\n",
    "df2 = df2.drop(\"vat_1\", \"lower_belt_window\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.withColumn(\"lower_belt_time_exit_time\", expr(\"cast(lower_belt_time as timestamp) + INTERVAL 60 MINUTES\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_spec = Window.orderBy(\"_time\")\n",
    "df2 = df2.withColumn(\"row_num\", row_number().over(window_spec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.withColumn(\"block_1\", \n",
    "                   when(col(\"row_num\") % 2 == 1, col(\"lower_belt_id\"))  # Odd rows go to block_1\n",
    "                   .otherwise(None))\n",
    "\n",
    "df2 = df2.withColumn(\"block_2\", \n",
    "                   when(col(\"row_num\") % 2 == 0, col(\"lower_belt_id\"))  # Even rows go to block_2\n",
    "                   .otherwise(None))\n",
    "df2 = df2.drop(\"row_num\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = df2.withColumn(\"lower_belt_time\", col(\"_time\") + expr(\"INTERVAL 30 MINUTES\"))\n",
    "\n",
    "# # Generate lower belt ID\n",
    "# df2 = df2.withColumn(\"lower_belt_time_window\", date_format(col(\"lower_belt_time\"), \"HHmmss\"))\n",
    "# df2 = df2.withColumn(\"lower_belt_id\", \n",
    "#                    concat_ws(\"\", \n",
    "#                              *[when(col(vat).isNotNull(), lit(f\"vat{i}_\"))\n",
    "#                              .otherwise(lit(\"\")) for i, vat in enumerate(vat_columns, 1)],\n",
    "#                              col(\"lower_belt_time_window\")\n",
    "#                    ))\n",
    "\n",
    "# # Remove empty strings from the lower belt ID\n",
    "# df2 = df2.withColumn(\"lower_belt_id\", \n",
    "#                    concat_ws(\"\", \n",
    "#                              *[col(\"lower_belt_id\").substr(i, 4) for i in range(1, 29, 4)]\n",
    "#                    ))\n",
    "\n",
    "# # Add lower belt ID\n",
    "# df2 = df2.withColumn(\"lower_belt_id\", concat_ws(\"_\", col(\"lower_belt_id\"), col(\"batch_id\")))\n",
    "\n",
    "# # Assign lower belt ID to the corresponding row\n",
    "# df2 = df2.withColumn(\"lower_belt_id\", \n",
    "#                    when(col(\"_time\") == col(\"lower_belt_time\"), col(\"lower_belt_id\"))\n",
    "#                    .otherwise(None))\n",
    "\n",
    "# # Show the result\n",
    "# df2.select(\"_time\", \"upper_belt_id\", \"lower_belt_time\", \"lower_belt_id\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2.select('VatNo1','VatNo2','VatNo3','VatNo4','VatNo5','VatNo6','VatNo7','_time','time_window','batch_start_timAe','batch_time_str','id','upper_belt_id').show(50,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/07 22:45:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/07 22:45:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/07 22:45:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/07 22:45:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/07 22:45:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/07 22:45:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/07 22:45:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/07 22:45:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/07 22:45:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/07 22:45:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/07 22:45:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/07 22:45:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/07 22:45:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/07 22:45:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/07 22:45:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/07 22:45:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/07 22:45:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/07 22:45:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/07 23:56:11 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 1082183 ms exceeds timeout 120000 ms\n",
      "25/03/07 23:56:11 WARN SparkContext: Killing executors is not supported by current scheduler.\n",
      "25/03/07 23:56:14 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.93:59646\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "25/03/07 23:56:14 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.93:59646\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "25/03/07 23:56:24 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.93:59646\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "25/03/07 23:56:24 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.1.93:59646\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n"
     ]
    }
   ],
   "source": [
    "blob_path = '/Users/sushantshinde/workspaces/trvi/output'\n",
    "df2.repartition(1).orderBy('_time') \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .csv(blob_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
